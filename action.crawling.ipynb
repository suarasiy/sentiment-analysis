{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from util import noValueBuild, getSingleValueInt\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from typing import TypedDict\n",
    "    from selenium.webdriver.remote.webelement import WebElement\n",
    "    from selenium.webdriver.remote.webdriver import WebDriver\n",
    "    from interface import Review, CrawlOptions, NamedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "#                                 Utils Driver                                 #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "def driverUp () -> WebDriver:\n",
    "    # set browser locale\n",
    "    browser_locale = \"en_EN\"\n",
    "\n",
    "    options = Options()\n",
    "    prefs = {\n",
    "        \"profile.default_content_setting_values.geolocation\": 2\n",
    "    }\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"--lang={}\".format(browser_locale))\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    # manually using ttw instead of WebDriverWait (must be note it also depends on your internet)\n",
    "    TIME_TO_WAIT = 1.45\n",
    "\n",
    "    # config for scrolling, if fail to fulfilled by the logic with maximum attempt\n",
    "    TIMEOUT_ATTEMPT = 5\n",
    "\n",
    "    driver = webdriver.Chrome(options)\n",
    "\n",
    "    # make life easier ;)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultOptions: CrawlOptions = {\n",
    "    \"CRAWL_NAME\"\n",
    "    \"TIME_TO_WAIT\": 1.45\n",
    "}\n",
    "\n",
    "def crawl (driver: WebDriver, url: str, show_result: bool = False, options: CrawlOptions = defaultOptions) -> list[Review]:\n",
    "    driver.get(url);\n",
    "    driver.implicitly_wait(1.1);\n",
    "\n",
    "    dom_review_pane = driver.find_elements(by=By.CSS_SELECTOR, value=\"[aria-label='Refine reviews']\")\n",
    "\n",
    "    dataReviews: list[Review] = []\n",
    "\n",
    "    # container scrollable\n",
    "    dom_review_container: WebElement | None = driver.execute_script(\"\"\"\n",
    "        return document.querySelector('[role=\"main\"]')?.children[1]\n",
    "    \"\"\")\n",
    "\n",
    "    if dom_review_container:\n",
    "        print('[CRAWL.Initialize: {}]: Container initialized. {}'.format(options.get(\"CRAWL_NAME\", \"-\"), dom_review_container.get_property(name=\"scrollHeight\")))\n",
    "        fst = True\n",
    "        while fst:\n",
    "            time.sleep(options.get(\"TIME_TO_WAIT\", 1))\n",
    "\n",
    "            sT = driver.execute_script(\"\"\"\n",
    "                return document.querySelector('[role=\"main\"]')?.children[1].scrollTop\n",
    "            \"\"\")\n",
    "            sH = driver.execute_script(\"\"\"\n",
    "                return document.querySelector('[role=\"main\"]')?.children[1].scrollHeight\n",
    "            \"\"\")\n",
    "            oH = driver.execute_script(\"\"\"\n",
    "                return document.querySelector('[role=\"main\"]')?.children[1].offsetHeight\n",
    "            \"\"\")\n",
    "            \n",
    "            if sT < (sH - oH) - 1:\n",
    "                driver.execute_script(\"\"\"\n",
    "                    return arguments[0].scrollTop = arguments[1]\n",
    "                \"\"\", dom_review_container, sH)\n",
    "                print(\"[CRAWL.Scroll]: Please wait... [{},{}]\".format(dom_review_container.get_property(name=\"scrollTop\"), sH))\n",
    "            else: fst = False\n",
    "\n",
    "    print(\"[CRAWL.Scroll]: Done.\")\n",
    "\n",
    "\n",
    "    dom_review_pane_by_rr = driver.execute_script(\"\"\"\n",
    "        return document.querySelector('[aria-label=\"Refine reviews\"]')\n",
    "    \"\"\")\n",
    "    dom_review_pane_by_cta_sort = driver.execute_script(\"\"\"\n",
    "        return document.querySelector('[aria-label=\"Sort reviews\"]')?.parentElement?.parentElement\n",
    "    \"\"\")\n",
    "\n",
    "    dom_review_pane_sibling = None\n",
    "\n",
    "    if dom_review_pane_by_rr or dom_review_pane_by_cta_sort:\n",
    "        dom_review_pane_sibling = driver.execute_script(\"\"\"\n",
    "            return arguments[0].nextElementSibling\n",
    "        \"\"\", dom_review_pane_by_rr if dom_review_pane_by_rr else dom_review_pane_by_cta_sort)\n",
    "\n",
    "    if dom_review_container and dom_review_pane_sibling:\n",
    "        dom_reviews = driver.execute_script(\"\"\"\n",
    "            return arguments[0].querySelectorAll(\":scope > [data-review-id]\")\n",
    "        \"\"\", dom_review_pane_sibling)\n",
    "\n",
    "        if len(dom_reviews) <= 0: return []\n",
    "\n",
    "        if len(dom_reviews) >= 0:\n",
    "            for review in dom_reviews:\n",
    "                authorName, authorContrib, authorReview, ariaStars, authorTimestamp, reviewImages = None, None, None, None, None, []\n",
    "                \n",
    "                src = driver.execute_script(\"\"\"\n",
    "                    return arguments[0].querySelector(\"[data-review-id]\")?.firstElementChild\n",
    "                \"\"\", review)\n",
    "                if src:\n",
    "                    # get author's name\n",
    "                    authorName = driver.execute_script(\"\"\"\n",
    "                        return arguments[0].children[1]?.querySelector(\"[data-review-id]\")?.children[0]?.textContent\n",
    "                    \"\"\", src)\n",
    "\n",
    "                    # get author's contrib\n",
    "                    authorContrib = driver.execute_script(\"\"\"\n",
    "                        return arguments[0].children[1]?.querySelector(\"[data-review-id]\")?.children[1]?.textContent\n",
    "                    \"\"\", src)\n",
    "\n",
    "                    # get stars aria-label\n",
    "                    ariaStars = driver.execute_script(\"\"\"\n",
    "                        return arguments[0].children[3]?.firstElementChild?.children[0]?.getAttribute(\"aria-label\")\n",
    "                    \"\"\", src)\n",
    "\n",
    "                    # get humanized_timestamp value\n",
    "                    authorTimestamp = driver.execute_script(\"\"\"\n",
    "                        return arguments[0].children[3]?.firstElementChild?.children[1]?.textContent\n",
    "                    \"\"\", src)\n",
    "\n",
    "                    # get review\n",
    "                    com_reviews = driver.execute_script(\"\"\"\n",
    "                        return arguments[0].children[3]?.children[1]?.querySelector(\"[id]\")?.children\n",
    "                    \"\"\", src)\n",
    "                    if com_reviews and len(com_reviews) > 0:\n",
    "                        if len(com_reviews) > 1:\n",
    "                            driver.execute_script(\"\"\"\n",
    "                                return arguments[0].firstElementChild?.click()\n",
    "                            \"\"\", com_reviews[1])\n",
    "                        authorReview = driver.execute_script(\"\"\"\n",
    "                            return arguments[0].textContent\n",
    "                        \"\"\", com_reviews[0])\n",
    "\n",
    "                    # get review images\n",
    "                    domImages: list[WebElement] | None = driver.execute_script(\"\"\"\n",
    "                        return arguments[0].children[3].querySelectorAll(\"[data-photo-index]\")\n",
    "                    \"\"\", src)\n",
    "                    if domImages and len(domImages) > 0:\n",
    "                        for image in domImages:\n",
    "                            style = image.get_attribute(\"style\")\n",
    "                            urls = re.findall(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\", style)\n",
    "                            reviewImages.append([x[0] for x in urls][0])\n",
    "\n",
    "                dataReviews.append({\n",
    "                    \"name\": noValueBuild(authorName, \"author\"),\n",
    "                    \"review\": noValueBuild(authorReview, \"review\"),\n",
    "                    \"contrib\": noValueBuild(authorContrib, \"contrib\"),\n",
    "                    \"humanized_timestamp\": noValueBuild(authorTimestamp, \"humanized_timestamp\"),\n",
    "                    \"stars\": {\n",
    "                        \"label\": noValueBuild(ariaStars, \"stars\"),\n",
    "                        \"value\": getSingleValueInt(ariaStars, 0),\n",
    "                    },\n",
    "                    \"minires_images\": noValueBuild(reviewImages, \"minires_images\")\n",
    "                })\n",
    "\n",
    "    print(\"[CRAWL.Finished] Process finished. Thank you.\")\n",
    "\n",
    "    if show_result:\n",
    "        print(json.dumps(dataReviews, indent=2, sort_keys=False))\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return dataReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from endpoint import urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_datas: list[NamedDataset] = []\n",
    "\n",
    "for url in urls():\n",
    "    driver = driverUp()\n",
    "    result = crawl(driver=driver, url=url.get(\"url\", \"\"), options={\n",
    "        \"CRAWL_NAME\": url.get(\"filename\", \"\"),\n",
    "        \"TIME_TO_WAIT\": 2.35\n",
    "    })\n",
    "    big_datas.append({\n",
    "        \"label\": url.get(\"filename\", \"\"),\n",
    "        \"dataset\": result\n",
    "    })\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataset(s) to csv through iteration\n",
    "for dataset in big_datas:\n",
    "    df = pd.json_normalize(dataset.get(\"dataset\", ))\n",
    "    df.to_csv(path_or_buf=\"./datasets/{}.csv\".format(dataset.get(\"label\", \"_\")), encoding=\"utf-8\", index=False, header=True)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\suarasiy\\skripsi\\datasets\n"
     ]
    }
   ],
   "source": [
    "path_to_datasets = \"{}\\\\datasets\".format(os.getcwd())\n",
    "all_dataset = (pd.read_csv(f) for f in glob(os.path.join(path_to_datasets, \"*.csv\")))\n",
    "dfs = pd.concat(all_dataset, ignore_index=True)\n",
    "dfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
