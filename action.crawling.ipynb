{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from util import noValueBuild, getSingleValueInt\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from selenium.webdriver.remote.webelement import WebElement\n",
    "    from selenium.webdriver.remote.webdriver import WebDriver\n",
    "    from interface import Review, CrawlOptions, NamedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "#                                 Utils Driver                                 #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "def driverUp () -> WebDriver:\n",
    "    # set browser locale\n",
    "    browser_locale = \"en_EN\"\n",
    "\n",
    "    options = Options()\n",
    "    prefs = {\n",
    "        \"profile.default_content_setting_values.geolocation\": 2\n",
    "    }\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"--lang={}\".format(browser_locale))\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    # manually using ttw instead of WebDriverWait (must be note it also depends on your internet)\n",
    "    TIME_TO_WAIT = 1.45\n",
    "\n",
    "    # config for scrolling, if fail to fulfilled by the logic with maximum attempt\n",
    "    TIMEOUT_ATTEMPT = 5\n",
    "\n",
    "    driver = webdriver.Chrome(options)\n",
    "\n",
    "    # make life easier ;)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultOptions: CrawlOptions = {\n",
    "    \"CRAWL_NAME\"\n",
    "    \"TIME_TO_WAIT\": 1.45\n",
    "}\n",
    "\n",
    "def crawl (driver: WebDriver, url: str, show_result: bool = False, options: CrawlOptions = defaultOptions) -> list[Review]:\n",
    "    driver.get(url);\n",
    "    driver.implicitly_wait(1.1);\n",
    "\n",
    "    # execution time for the whole process\n",
    "    start = time.time()\n",
    "\n",
    "    dom_review_pane = driver.find_elements(by=By.CSS_SELECTOR, value=\"[aria-label='Refine reviews']\")\n",
    "\n",
    "    dataReviews: list[Review] = []\n",
    "\n",
    "    # container scrollable\n",
    "    dom_review_container: WebElement | None = driver.execute_script(\"\"\"\n",
    "        return document.querySelector('[role=\"main\"]')?.children[1]\n",
    "    \"\"\")\n",
    "\n",
    "    if dom_review_container:\n",
    "        print('[CRAWL.Initialize: {}]: Container initialized. {}'.format(options.get(\"CRAWL_NAME\", \"-\"), dom_review_container.get_property(name=\"scrollHeight\")))\n",
    "        fst = True\n",
    "        while fst:\n",
    "            time.sleep(options.get(\"TIME_TO_WAIT\", 1))\n",
    "\n",
    "            sT = driver.execute_script(\"\"\"\n",
    "                return document.querySelector('[role=\"main\"]')?.children[1].scrollTop\n",
    "            \"\"\")\n",
    "            sH = driver.execute_script(\"\"\"\n",
    "                return document.querySelector('[role=\"main\"]')?.children[1].scrollHeight\n",
    "            \"\"\")\n",
    "            oH = driver.execute_script(\"\"\"\n",
    "                return document.querySelector('[role=\"main\"]')?.children[1].offsetHeight\n",
    "            \"\"\")\n",
    "            \n",
    "            if sT < (sH - oH) - 1:\n",
    "                driver.execute_script(\"\"\"\n",
    "                    return arguments[0].scrollTop = arguments[1]\n",
    "                \"\"\", dom_review_container, sH)\n",
    "                print(\"[CRAWL.Scroll]: Please wait... [{},{}]\".format(dom_review_container.get_property(name=\"scrollTop\"), sH))\n",
    "            else: fst = False\n",
    "\n",
    "    print(\"[CRAWL.Scroll]: Done.\")\n",
    "\n",
    "\n",
    "    dom_review_pane_by_rr = driver.execute_script(\"\"\"\n",
    "        return document.querySelector('[aria-label=\"Refine reviews\"]')\n",
    "    \"\"\")\n",
    "    dom_review_pane_by_cta_sort = driver.execute_script(\"\"\"\n",
    "        return document.querySelector('[aria-label=\"Sort reviews\"]')?.parentElement?.parentElement\n",
    "    \"\"\")\n",
    "\n",
    "    dom_review_pane_sibling = None\n",
    "\n",
    "    if dom_review_pane_by_rr or dom_review_pane_by_cta_sort:\n",
    "        dom_review_pane_sibling = driver.execute_script(\"\"\"\n",
    "            return arguments[0].nextElementSibling\n",
    "        \"\"\", dom_review_pane_by_rr if dom_review_pane_by_rr else dom_review_pane_by_cta_sort)\n",
    "\n",
    "    if dom_review_container and dom_review_pane_sibling:\n",
    "        dom_reviews = driver.execute_script(\"\"\"\n",
    "            return arguments[0].querySelectorAll(\":scope > [data-review-id]\")\n",
    "        \"\"\", dom_review_pane_sibling)\n",
    "\n",
    "        if len(dom_reviews) <= 0: return []\n",
    "\n",
    "        if len(dom_reviews) >= 0:\n",
    "            for review in dom_reviews:\n",
    "                authorName, authorContrib, authorReview, ariaStars, authorTimestamp, reviewImages = None, None, None, None, None, []\n",
    "                \n",
    "                src = driver.execute_script(\"\"\"\n",
    "                    return arguments[0].querySelector(\"[data-review-id]\")?.firstElementChild\n",
    "                \"\"\", review)\n",
    "                if src:\n",
    "                    # get author's name\n",
    "                    authorName = driver.execute_script(\"\"\"\n",
    "                        return arguments[0].children[1]?.querySelector(\"[data-review-id]\")?.children[0]?.textContent\n",
    "                    \"\"\", src)\n",
    "\n",
    "                    # get author's contrib\n",
    "                    authorContrib = driver.execute_script(\"\"\"\n",
    "                        return arguments[0].children[1]?.querySelector(\"[data-review-id]\")?.children[1]?.textContent\n",
    "                    \"\"\", src)\n",
    "\n",
    "                    # get stars aria-label\n",
    "                    ariaStars = driver.execute_script(\"\"\"\n",
    "                        return arguments[0].children[3]?.firstElementChild?.children[0]?.getAttribute(\"aria-label\")\n",
    "                    \"\"\", src)\n",
    "\n",
    "                    # get humanized_timestamp value\n",
    "                    authorTimestamp = driver.execute_script(\"\"\"\n",
    "                        return arguments[0].children[3]?.firstElementChild?.children[1]?.textContent\n",
    "                    \"\"\", src)\n",
    "\n",
    "                    # get review\n",
    "                    com_reviews = driver.execute_script(\"\"\"\n",
    "                        return arguments[0].children[3]?.children[1]?.querySelector(\"[id]\")?.children\n",
    "                    \"\"\", src)\n",
    "                    if com_reviews and len(com_reviews) > 0:\n",
    "                        if len(com_reviews) > 1:\n",
    "                            driver.execute_script(\"\"\"\n",
    "                                return arguments[0].firstElementChild?.click()\n",
    "                            \"\"\", com_reviews[1])\n",
    "                        authorReview = driver.execute_script(\"\"\"\n",
    "                            return arguments[0].textContent\n",
    "                        \"\"\", com_reviews[0])\n",
    "\n",
    "                    # get review images\n",
    "                    domImages: list[WebElement] | None = driver.execute_script(\"\"\"\n",
    "                        return arguments[0].children[3].querySelectorAll(\"[data-photo-index]\")\n",
    "                    \"\"\", src)\n",
    "                    if domImages and len(domImages) > 0:\n",
    "                        for image in domImages:\n",
    "                            style = image.get_attribute(\"style\")\n",
    "                            urls = re.findall(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\", style)\n",
    "                            reviewImages.append([x[0] for x in urls][0])\n",
    "\n",
    "                dataReviews.append({\n",
    "                    \"name\": noValueBuild(authorName, \"author\"),\n",
    "                    \"review\": noValueBuild(authorReview, \"review\"),\n",
    "                    \"contrib\": noValueBuild(authorContrib, \"contrib\"),\n",
    "                    \"humanized_timestamp\": noValueBuild(authorTimestamp, \"humanized_timestamp\"),\n",
    "                    \"stars\": {\n",
    "                        \"label\": noValueBuild(ariaStars, \"stars\"),\n",
    "                        \"value\": getSingleValueInt(ariaStars, 0),\n",
    "                    },\n",
    "                    \"minires_images\": noValueBuild(reviewImages, \"minires_images\")\n",
    "                })\n",
    "\n",
    "    print(\"[CRAWL.Finished] Process finished. Thank you.\")\n",
    "\n",
    "    if show_result:\n",
    "        print(json.dumps(dataReviews, indent=2, sort_keys=False))\n",
    "\n",
    "    # evaluate the time taken\n",
    "    print(\"[CRAWL.Time]: Execution time: {:.2f} seconds\".format(time.time() - start))\n",
    "    print(\"Total reviews: {}\".format(len(dataReviews)))\n",
    "    print(\"=====================================================================================================\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return dataReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from endpoint import urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CRAWL.Initialize: dataset_jembatan_penyebrangan_pulau_kumala]: Container initialized. 5055\n",
      "[CRAWL.Scroll]: Please wait... [4380,5055]\n",
      "[CRAWL.Scroll]: Please wait... [9005.599609375,9681]\n",
      "[CRAWL.Scroll]: Please wait... [12596.7998046875,13272]\n",
      "[CRAWL.Scroll]: Please wait... [15999.2001953125,16674]\n",
      "[CRAWL.Scroll]: Please wait... [19972,20647]\n",
      "[CRAWL.Scroll]: Please wait... [23564.80078125,24240]\n",
      "[CRAWL.Scroll]: Please wait... [26820,27495]\n",
      "[CRAWL.Scroll]: Please wait... [29780,30455]\n",
      "[CRAWL.Scroll]: Please wait... [31768,32443]\n",
      "[CRAWL.Scroll]: Please wait... [33756,34431]\n",
      "[CRAWL.Scroll]: Please wait... [36060.80078125,36736]\n",
      "[CRAWL.Scroll]: Please wait... [38048.80078125,38724]\n",
      "[CRAWL.Scroll]: Please wait... [40036.80078125,40712]\n",
      "[CRAWL.Scroll]: Please wait... [42341.6015625,43016]\n",
      "[CRAWL.Scroll]: Please wait... [44329.6015625,45004]\n",
      "[CRAWL.Scroll]: Please wait... [50511.19921875,51186]\n",
      "[CRAWL.Scroll]: Please wait... [56502.3984375,57178]\n",
      "[CRAWL.Scroll]: Please wait... [62810.3984375,63486]\n",
      "[CRAWL.Scroll]: Please wait... [68971.203125,69647]\n",
      "[CRAWL.Scroll]: Please wait... [74816,75491]\n",
      "[CRAWL.Scroll]: Please wait... [81060.796875,81736]\n",
      "[CRAWL.Scroll]: Please wait... [86968,87643]\n",
      "[CRAWL.Scroll]: Please wait... [92707.203125,93382]\n",
      "[CRAWL.Scroll]: Please wait... [98574.3984375,99249]\n",
      "[CRAWL.Scroll]: Please wait... [103616.796875,104292]\n",
      "[CRAWL.Scroll]: Please wait... [108680,109355]\n",
      "[CRAWL.Scroll]: Please wait... [113997.6015625,114673]\n",
      "[CRAWL.Scroll]: Please wait... [118532.796875,119208]\n",
      "[CRAWL.Scroll]: Please wait... [122962.3984375,123638]\n",
      "[CRAWL.Scroll]: Please wait... [126587.203125,127263]\n",
      "[CRAWL.Scroll]: Please wait... [131053.6015625,131729]\n",
      "[CRAWL.Scroll]: Please wait... [134489.59375,135164]\n",
      "[CRAWL.Scroll]: Please wait... [138490.40625,139166]\n",
      "[CRAWL.Scroll]: Done.\n",
      "[CRAWL.Finished] Process finished. Thank you.\n",
      "[CRAWL.Time]: Execution time: 93.61 seconds\n",
      "Total reviews: 328\n",
      "=====================================================================================================\n",
      "[CRAWL.Initialize: dataset_wisata_tenggarong_kolam_naga]: Container initialized. 3488\n",
      "[CRAWL.Scroll]: Please wait... [2812.800048828125,3488]\n",
      "[CRAWL.Scroll]: Please wait... [8251.2001953125,8927]\n",
      "[CRAWL.Scroll]: Please wait... [12653.599609375,13329]\n",
      "[CRAWL.Scroll]: Please wait... [15902.400390625,16577]\n",
      "[CRAWL.Scroll]: Please wait... [18749.599609375,19425]\n",
      "[CRAWL.Scroll]: Please wait... [21918.400390625,22593]\n",
      "[CRAWL.Scroll]: Please wait... [25355.19921875,26031]\n",
      "[CRAWL.Scroll]: Please wait... [28140,28815]\n",
      "[CRAWL.Scroll]: Please wait... [29508,30183]\n",
      "[CRAWL.Scroll]: Please wait... [30876,31551]\n",
      "[CRAWL.Scroll]: Please wait... [32546.400390625,33221]\n",
      "[CRAWL.Scroll]: Please wait... [34024,34699]\n",
      "[CRAWL.Scroll]: Please wait... [35392,36067]\n",
      "[CRAWL.Scroll]: Please wait... [36992,37667]\n",
      "[CRAWL.Scroll]: Done.\n",
      "[CRAWL.Finished] Process finished. Thank you.\n",
      "[CRAWL.Time]: Execution time: 39.15 seconds\n",
      "Total reviews: 138\n",
      "=====================================================================================================\n",
      "[CRAWL.Initialize: dataset_sky_tower_kumala]: Container initialized. 2157\n",
      "[CRAWL.Scroll]: Please wait... [0,2157]\n",
      "[CRAWL.Scroll]: Please wait... [1412,2087]\n",
      "[CRAWL.Scroll]: Done.\n",
      "[CRAWL.Finished] Process finished. Thank you.\n",
      "[CRAWL.Time]: Execution time: 8.54 seconds\n",
      "Total reviews: 8\n",
      "=====================================================================================================\n",
      "[CRAWL.Initialize: dataset_kumala_central_park]: Container initialized. 674\n",
      "[CRAWL.Scroll]: Done.\n",
      "[CRAWL.Finished] Process finished. Thank you.\n",
      "[CRAWL.Time]: Execution time: 3.57 seconds\n",
      "Total reviews: 1\n",
      "=====================================================================================================\n",
      "[CRAWL.Initialize: dataset_arena_balap_gokart]: Container initialized. 674\n",
      "[CRAWL.Scroll]: Done.\n",
      "[CRAWL.Finished] Process finished. Thank you.\n",
      "[CRAWL.Time]: Execution time: 3.55 seconds\n",
      "Total reviews: 1\n",
      "=====================================================================================================\n",
      "[CRAWL.Initialize: dataset_kumala_island]: Container initialized. 4841\n",
      "[CRAWL.Scroll]: Please wait... [4165.60009765625,4841]\n",
      "[CRAWL.Scroll]: Please wait... [9564.7998046875,10240]\n",
      "[CRAWL.Scroll]: Please wait... [13170.400390625,13846]\n",
      "[CRAWL.Scroll]: Please wait... [15932.7998046875,16608]\n",
      "[CRAWL.Scroll]: Please wait... [18784,19459]\n",
      "[CRAWL.Scroll]: Please wait... [20688.80078125,21364]\n",
      "[CRAWL.Scroll]: Please wait... [21576.80078125,22252]\n",
      "[CRAWL.Scroll]: Done.\n",
      "[CRAWL.Finished] Process finished. Thank you.\n",
      "[CRAWL.Time]: Execution time: 37.36 seconds\n",
      "Total reviews: 65\n",
      "=====================================================================================================\n",
      "[CRAWL.Initialize: dataset_candi_naga_lembuswana]: Container initialized. 674\n",
      "[CRAWL.Scroll]: Done.\n",
      "[CRAWL.Finished] Process finished. Thank you.\n",
      "[CRAWL.Time]: Execution time: 3.54 seconds\n",
      "Total reviews: 1\n",
      "=====================================================================================================\n",
      "[CRAWL.Initialize: dataset_dayak_experience_center]: Container initialized. 3564\n",
      "[CRAWL.Scroll]: Please wait... [2889.60009765625,3564]\n",
      "[CRAWL.Scroll]: Please wait... [4767.2001953125,5442]\n",
      "[CRAWL.Scroll]: Please wait... [5654.39990234375,6330]\n",
      "[CRAWL.Scroll]: Done.\n",
      "[CRAWL.Finished] Process finished. Thank you.\n",
      "[CRAWL.Time]: Execution time: 10.43 seconds\n",
      "Total reviews: 25\n",
      "=====================================================================================================\n",
      "[CRAWL.Initialize: dataset_pulau_kumala]: Container initialized. 3842\n",
      "[CRAWL.Scroll]: Please wait... [3167.199951171875,3842]\n",
      "[CRAWL.Scroll]: Please wait... [5641.60009765625,6316]\n",
      "[CRAWL.Scroll]: Please wait... [7461.60009765625,8136]\n",
      "[CRAWL.Scroll]: Please wait... [9327.2001953125,10002]\n",
      "[CRAWL.Scroll]: Please wait... [10020,10695]\n",
      "[CRAWL.Scroll]: Done.\n",
      "[CRAWL.Finished] Process finished. Thank you.\n",
      "[CRAWL.Time]: Execution time: 16.67 seconds\n",
      "Total reviews: 43\n",
      "=====================================================================================================\n",
      "[CRAWL.Initialize: dataset_patung_lembuswana_area_sps]: Container initialized. 3618\n",
      "[CRAWL.Scroll]: Please wait... [2943.199951171875,3618]\n",
      "[CRAWL.Scroll]: Please wait... [4832.7998046875,5508]\n",
      "[CRAWL.Scroll]: Please wait... [6200.7998046875,6876]\n",
      "[CRAWL.Scroll]: Please wait... [6268,6943]\n",
      "[CRAWL.Scroll]: Done.\n",
      "[CRAWL.Finished] Process finished. Thank you.\n",
      "[CRAWL.Time]: Execution time: 13.79 seconds\n",
      "Total reviews: 29\n",
      "=====================================================================================================\n",
      "[CRAWL.Initialize: dataset_taman_air_mancur_pulau_kumala]: Container initialized. 674\n",
      "[CRAWL.Scroll]: Done.\n",
      "[CRAWL.Finished] Process finished. Thank you.\n",
      "[CRAWL.Time]: Execution time: 3.56 seconds\n",
      "Total reviews: 1\n",
      "=====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "big_datas: list[NamedDataset] = []\n",
    "\n",
    "for url in urls():\n",
    "    driver = driverUp()\n",
    "    result = crawl(driver=driver, url=url.get(\"url\", \"\"), options={\n",
    "        \"CRAWL_NAME\": url.get(\"filename\", \"\"),\n",
    "        \"TIME_TO_WAIT\": 2.35\n",
    "    })\n",
    "    big_datas.append({\n",
    "        \"label\": url.get(\"filename\", \"\"),\n",
    "        \"dataset\": result\n",
    "    })\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataset(s) to csv through iteration\n",
    "for dataset in big_datas:\n",
    "    df = pd.json_normalize(dataset.get(\"dataset\", ))\n",
    "    df.to_csv(path_or_buf=\"./datasets/{}.csv\".format(dataset.get(\"label\", \"_\")), encoding=\"utf-8\", index=False, header=True)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\suarasiy\\skripsi\\datasets\n"
     ]
    }
   ],
   "source": [
    "path_to_datasets = \"{}\\\\datasets\".format(os.getcwd())\n",
    "all_dataset = (pd.read_csv(f) for f in glob(os.path.join(path_to_datasets, \"*.csv\")))\n",
    "dfs = pd.concat(all_dataset, ignore_index=True)\n",
    "dfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
